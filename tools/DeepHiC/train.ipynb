{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import visdom\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from models.deephic import Generator, Discriminator\n",
    "from models.loss import GeneratorLoss\n",
    "from models.ssim import ssim\n",
    "from math import log10\n",
    "\n",
    "from all_parser import root_dir\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = np.column_stack\n",
    "\n",
    "# data_dir: directory storing processed data\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "# out_dir: directory storing checkpoint files\n",
    "out_dir = os.path.join(root_dir, 'checkpoints')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "datestr = time.strftime('%m_%d_%H_%M')\n",
    "visdom_str=time.strftime('%m%d')\n",
    "\n",
    "resos = '10kb40kb' \n",
    "chunk = 40\n",
    "stride = 40\n",
    "bound = 201\n",
    "pool = 'nonpool'\n",
    "\n",
    "upscale = 1\n",
    "#FIXME\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# whether using GPU for training\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([53238, 1, 40, 40])\n",
      "torch.Size([18801, 1, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "#数据\n",
    "# prepare training dataset\n",
    "train_file = os.path.join(data_dir, f'deephic_{resos}_c{chunk}_s{stride}_b{bound}_{pool}_train.npz')\n",
    "train = np.load(train_file)\n",
    "#data和target的size都是40*40的\n",
    "train_data = torch.tensor(train['data'], dtype=torch.float)\n",
    "train_target = torch.tensor(train['target'], dtype=torch.float)\n",
    "train_inds = torch.tensor(train['inds'], dtype=torch.long) #这个是什么意思\n",
    "\n",
    "train_set = TensorDataset(train_data, train_target, train_inds)\n",
    "\n",
    "# prepare valid dataset\n",
    "valid_file = os.path.join(data_dir, f'deephic_{resos}_c{chunk}_s{stride}_b{bound}_{pool}_valid.npz')\n",
    "valid = np.load(valid_file)\n",
    "\n",
    "valid_data = torch.tensor(valid['data'], dtype=torch.float)\n",
    "valid_target = torch.tensor(valid['target'], dtype=torch.float)\n",
    "valid_inds = torch.tensor(valid['inds'], dtype=torch.long)\n",
    "\n",
    "valid_set = TensorDataset(valid_data, valid_target, valid_inds)\n",
    "\n",
    "# DataLoader for batched training\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/home/mliu/anaconda3/envs/sv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/share/home/mliu/anaconda3/envs/sv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load network\n",
    "netG = Generator(upscale, in_channel=1, resblock_num=5).to(device)\n",
    "netD = Discriminator(in_channel=1).to(device)\n",
    "\n",
    "# loss function\n",
    "#非常复杂的损失函数\n",
    "criterionG = GeneratorLoss().to(device)\n",
    "criterionD = torch.nn.BCELoss().to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0001)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(env=f'{visdom_str}-deephic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fruitpunch = sns.blend_palette(['white', 'red'], as_cmap=True)\n",
    "# for data, target, _ in train_loader:\n",
    "#     print(data.shape)\n",
    "#     print(target.shape)\n",
    "#     print(_.shape)\n",
    "#     idx = 3\n",
    "#     low_picture = data[idx].numpy().reshape(40,40)\n",
    "#     high_picture = target[idx].numpy().reshape(40,40)\n",
    "    \n",
    "#     #画low picture和high picture的heatmap\n",
    "#     fig,ax = plt.subplots(figsize=(20,10),ncols=2)\n",
    "#     im = ax[0].matshow(\n",
    "#         low_picture,\n",
    "#         vmin=0,\n",
    "#         cmap=fruitpunch)\n",
    "#     plt.colorbar(im, ax=ax[0] ,fraction=0.046, pad=0.04, label='raw counts');\n",
    "#     im = ax[1].matshow(\n",
    "#         high_picture,\n",
    "#         vmin=0,\n",
    "#         cmap=fruitpunch)\n",
    "#     plt.colorbar(im, ax=ax[1] ,fraction=0.046, pad=0.04, label='raw counts');\n",
    "#     plt.show()\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/10] Loss_D: 0.4621 Loss_G: 0.0026 D(x): 0.8051 D(G(z)): 0.1955: 100%|██████████| 831/831 [23:15<00:00,  1.68s/it]\n",
      "[Predicting in Test set] PSNR: 32.5985 dB SSIM: 0.8439: 100%|██████████| 293/293 [02:11<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, Best ssim is 0.843892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/10] Loss_D: 0.0789 Loss_G: 0.0006 D(x): 0.9618 D(G(z)): 0.0382: 100%|██████████| 831/831 [21:22<00:00,  1.54s/it]\n",
      "[Predicting in Test set] PSNR: 33.4522 dB SSIM: 0.8763: 100%|██████████| 293/293 [01:38<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, Best ssim is 0.876289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/10] Loss_D: 0.0281 Loss_G: 0.0005 D(x): 0.9862 D(G(z)): 0.0138: 100%|██████████| 831/831 [19:59<00:00,  1.44s/it]\n",
      "[Predicting in Test set] PSNR: 33.6064 dB SSIM: 0.8626: 100%|██████████| 293/293 [01:39<00:00,  2.96it/s]\n",
      "[4/10] Loss_D: 0.0343 Loss_G: 0.0005 D(x): 0.9870 D(G(z)): 0.0133: 100%|██████████| 831/831 [20:14<00:00,  1.46s/it]\n",
      "[Predicting in Test set] PSNR: 33.8972 dB SSIM: 0.8605: 100%|██████████| 293/293 [01:43<00:00,  2.83it/s]\n",
      "[5/10] Loss_D: 0.0078 Loss_G: 0.0005 D(x): 0.9960 D(G(z)): 0.0038: 100%|██████████| 831/831 [20:22<00:00,  1.47s/it]\n",
      "[Predicting in Test set] PSNR: 33.9638 dB SSIM: 0.8712: 100%|██████████| 293/293 [01:43<00:00,  2.83it/s]\n",
      "[6/10] Loss_D: 0.1592 Loss_G: 0.0004 D(x): 0.9412 D(G(z)): 0.0583: 100%|██████████| 831/831 [20:21<00:00,  1.47s/it]\n",
      "[Predicting in Test set] PSNR: 33.9591 dB SSIM: 0.8746: 100%|██████████| 293/293 [01:37<00:00,  3.02it/s]\n",
      "[7/10] Loss_D: 0.0168 Loss_G: 0.0004 D(x): 0.9929 D(G(z)): 0.0095: 100%|██████████| 831/831 [20:21<00:00,  1.47s/it]\n",
      "[Predicting in Test set] PSNR: 34.0559 dB SSIM: 0.8455: 100%|██████████| 293/293 [01:38<00:00,  2.98it/s]\n",
      "[8/10] Loss_D: 0.0269 Loss_G: 0.0004 D(x): 0.9891 D(G(z)): 0.0110: 100%|██████████| 831/831 [20:15<00:00,  1.46s/it]\n",
      "[Predicting in Test set] PSNR: 32.9932 dB SSIM: 0.7014: 100%|██████████| 293/293 [01:38<00:00,  2.99it/s]\n",
      "[9/10] Loss_D: 0.0298 Loss_G: 0.0004 D(x): 0.9888 D(G(z)): 0.0110: 100%|██████████| 831/831 [20:22<00:00,  1.47s/it]\n",
      "[Predicting in Test set] PSNR: 34.2799 dB SSIM: 0.8800: 100%|██████████| 293/293 [01:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, Best ssim is 0.879968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/10] Loss_D: 0.0165 Loss_G: 0.0004 D(x): 0.9935 D(G(z)): 0.0070: 100%|██████████| 831/831 [21:06<00:00,  1.52s/it]\n",
      "[Predicting in Test set] PSNR: 34.3463 dB SSIM: 0.8736: 100%|██████████| 293/293 [01:51<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "best_ssim = 0\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    run_result = {'nsamples': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    train_bar = tqdm(train_loader)\n",
    "    for data, target, _ in train_bar:\n",
    "        batch_size = data.size(0)\n",
    "        run_result['nsamples'] += batch_size\n",
    "        ############################\n",
    "        # (1) Update D network: maximize D(x)-1-D(G(z))\n",
    "        ###########################\n",
    "        real_img = target.to(device)\n",
    "        z = data.to(device)\n",
    "        fake_img = netG(z)\n",
    "\n",
    "        ######### Train discriminator #########\n",
    "        netD.zero_grad()\n",
    "        real_out = netD(real_img)\n",
    "        fake_out = netD(fake_img)\n",
    "        d_loss_real = criterionD(real_out, torch.ones_like(real_out))\n",
    "        d_loss_fake = criterionD(fake_out, torch.zeros_like(fake_out))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "\n",
    "        ######### Train generator #########\n",
    "        netG.zero_grad()\n",
    "        g_loss = criterionG(fake_out.mean(), fake_img, real_img)\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        run_result['g_loss'] += g_loss.item() * batch_size\n",
    "        run_result['d_loss'] += d_loss.item() * batch_size\n",
    "        run_result['d_score'] += real_out.mean().item() * batch_size\n",
    "        run_result['g_score'] += fake_out.mean().item() * batch_size\n",
    "\n",
    "        train_bar.set_description(desc=f\"[{epoch}/{num_epochs}] Loss_D: {run_result['d_loss']/run_result['nsamples']:.4f} Loss_G: {run_result['g_loss']/run_result['nsamples']:.4f} D(x): {run_result['d_score']/run_result['nsamples']:.4f} D(G(z)): {run_result['g_score']/run_result['nsamples']:.4f}\")\n",
    "    train_gloss = run_result['g_loss']/run_result['nsamples']\n",
    "    train_dloss = run_result['d_loss']/run_result['nsamples']\n",
    "    train_dscore = run_result['d_score']/run_result['nsamples']\n",
    "    train_gscore = run_result['g_score']/run_result['nsamples']\n",
    "\n",
    "    valid_result = {'g_loss': 0, 'd_loss': 0, 'g_score': 0, 'd_score': 0, \n",
    "                    'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'nsamples': 0}\n",
    "    netG.eval()\n",
    "    netD.eval()\n",
    "    valid_bar = tqdm(valid_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_lr, val_hr, inds in valid_bar:\n",
    "            batch_size = val_lr.size(0)\n",
    "            valid_result['nsamples'] += batch_size\n",
    "            lr = val_lr.to(device)\n",
    "            hr = val_hr.to(device) #真的，target\n",
    "\n",
    "            #生成的，假的\n",
    "            sr = netG(lr)\n",
    "\n",
    "            sr_out = netD(sr)\n",
    "            hr_out = netD(hr)\n",
    "            d_loss_real = criterionD(hr_out, torch.ones_like(hr_out))\n",
    "            d_loss_fake = criterionD(sr_out, torch.zeros_like(sr_out))\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            g_loss = criterionG(sr_out.mean(), sr, hr)\n",
    "            \n",
    "            valid_result['g_loss'] += g_loss.item() * batch_size\n",
    "            valid_result['d_loss'] += d_loss.item() * batch_size\n",
    "            valid_result['g_score'] += sr_out.mean().item() * batch_size\n",
    "            valid_result['d_score'] += hr_out.mean().item() * batch_size\n",
    "\n",
    "            batch_mse = ((sr - hr) ** 2).mean()\n",
    "            valid_result['mse'] += batch_mse * batch_size\n",
    "            #计算sr和hr的ssim\n",
    "            batch_ssim = ssim(sr, hr)\n",
    "            valid_result['ssims'] += batch_ssim * batch_size\n",
    "            valid_result['psnr'] = 10 * log10(1/(valid_result['mse']/valid_result['nsamples']))\n",
    "            valid_result['ssim'] = valid_result['ssims'] / valid_result['nsamples']\n",
    "            valid_bar.set_description(desc=f\"[Predicting in Test set] PSNR: {valid_result['psnr']:.4f} dB SSIM: {valid_result['ssim']:.4f}\")\n",
    "\n",
    "    valid_gloss = valid_result['g_loss'] / valid_result['nsamples']\n",
    "    valid_dloss = valid_result['d_loss'] / valid_result['nsamples']\n",
    "    valid_gscore = valid_result['g_score'] / valid_result['nsamples']\n",
    "    valid_dscore = valid_result['d_score'] / valid_result['nsamples']\n",
    "    now_ssim = valid_result['ssim'].item()\n",
    "    \n",
    "    if epoch == 1:\n",
    "        #初始化\n",
    "        vis_dloss = vis.line(X=cs((epoch, epoch)), Y=cs((train_dloss, valid_dloss)), opts=dict(title='Discriminator Loss', legend=['Train', 'Test']))\n",
    "        vis_gloss = vis.line(X=cs((epoch, epoch)), Y=cs((train_gloss, valid_gloss)), opts=dict(title='Generator Loss', legend=['Train', 'Test']))\n",
    "        vis_dscore = vis.line(X=cs((epoch, epoch)), Y=cs((train_dscore, valid_dscore)), opts=dict(title='Discriminator Score of true images', legend=['Train', 'Test']))\n",
    "        vis_gscore = vis.line(X=cs((epoch, epoch)), Y=cs((train_gscore, valid_gscore)), opts=dict(title='Generator Score of fake images', legend=['Train', 'Test']))\n",
    "        vis_ssim = vis.line([now_ssim], X=[epoch], opts=dict(title='SSIM scores in test dataset'))\n",
    "    else:\n",
    "        #添加\n",
    "        vis.line(X=cs((epoch, epoch)), Y=cs((train_dloss, valid_dloss)), update='append', win=vis_dloss, opts=dict(legend=['Train', 'Test']))\n",
    "        vis.line(X=cs((epoch, epoch)), Y=cs((train_gloss, valid_gloss)), update='append', win=vis_gloss, opts=dict(legend=['Train', 'Test']))\n",
    "        vis.line(X=cs((epoch, epoch)), Y=cs((train_dscore, valid_dscore)), update='append', win=vis_dscore, opts=dict(legend=['Train', 'Test']))\n",
    "        vis.line(X=cs((epoch, epoch)), Y=cs((train_gscore, valid_gscore)), update='append', win=vis_gscore, opts=dict(legend=['Train', 'Test']))\n",
    "        vis.line([now_ssim], X=[epoch], update='append', win=vis_ssim)\n",
    "\n",
    "    if now_ssim > best_ssim:\n",
    "        best_ssim = now_ssim\n",
    "        print(f'Now, Best ssim is {best_ssim:.6f}')\n",
    "        #并没有保存这个啊\n",
    "        best_ckpt_file = f'{datestr}_bestg_{resos}_c{chunk}_s{stride}_b{bound}_{pool}_deephic.pytorch'\n",
    "        torch.save(netG.state_dict(), os.path.join(out_dir, best_ckpt_file))\n",
    "\n",
    "final_ckpt_g = f'{datestr}_finalg_{resos}_c{chunk}_s{stride}_b{bound}_{pool}_deephic.pytorch'\n",
    "final_ckpt_d = f'{datestr}_finald_{resos}_c{chunk}_s{stride}_b{bound}_{pool}_deephic.pytorch'\n",
    "\n",
    "\n",
    "#存储文件\n",
    "torch.save(netG.state_dict(), os.path.join(out_dir, final_ckpt_g))\n",
    "torch.save(netD.state_dict(), os.path.join(out_dir, final_ckpt_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 22102,     0,     0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
